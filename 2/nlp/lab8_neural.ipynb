{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1-4\n",
    "Configure a document store based on Faiss supported by multilingual E5 model.\n",
    "\n",
    "Load the documents (passages) from the FiQA corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\jupiterb\\agh-data-science\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from haystack.document_stores.faiss import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Nie mówię, że nie podoba mi się też pomysł szk...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td>Tak więc nic nie zapobiega fałszywym ocenom po...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>Nigdy nie możesz korzystać z FSA dla indywidua...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td></td>\n",
       "      <td>Samsung stworzył LCD i inne technologie płaski...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td></td>\n",
       "      <td>Oto wymagania SEC: Federalne przepisy dotycząc...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title                                               text metadata\n",
       "_id                                                                  \n",
       "3          Nie mówię, że nie podoba mi się też pomysł szk...       {}\n",
       "31         Tak więc nic nie zapobiega fałszywym ocenom po...       {}\n",
       "56         Nigdy nie możesz korzystać z FSA dla indywidua...       {}\n",
       "59         Samsung stworzył LCD i inne technologie płaski...       {}\n",
       "63         Oto wymagania SEC: Federalne przepisy dotycząc...       {}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.read_json(f\"./data/corpus.jsonl\", lines=True)\n",
    "corpus_df = corpus_df.set_index(\"_id\").sort_index()\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Co jest uważane za wydatek służbowy w podróży ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zgłaszanie wydatków biznesowych dla firmy bez ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Przekazywanie pieniędzy z jednej kontroli bizn...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Posiadanie oddzielnego konta bankowego do prow...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wydatki służbowe - ubezpieczenie samochodu pod...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text metadata\n",
       "_id                                                            \n",
       "0    Co jest uważane za wydatek służbowy w podróży ...       {}\n",
       "1    Zgłaszanie wydatków biznesowych dla firmy bez ...       {}\n",
       "2    Przekazywanie pieniędzy z jednej kontroli bizn...       {}\n",
       "3    Posiadanie oddzielnego konta bankowego do prow...       {}\n",
       "4    Wydatki służbowe - ubezpieczenie samochodu pod...       {}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df = pd.read_json(f\"./data/queries.jsonl\", lines=True)\n",
    "queries_df = queries_df.set_index(\"_id\").sort_index()\n",
    "queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test positive examples: 1706\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>566392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>65404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>325273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>88124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>285255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id  corpus-id  score\n",
       "0         8     566392      1\n",
       "1         8      65404      1\n",
       "2        15     325273      1\n",
       "3        18      88124      1\n",
       "4        26     285255      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_test_df = pd.read_csv(f\"./data/test.tsv\", sep=\"\\t\")\n",
    "print(f\"Number of test positive examples: {len(qa_test_df)}\")\n",
    "\n",
    "qa_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_ids = set([row[\"corpus-id\"] for _, row in qa_test_df.iterrows()])\n",
    "test_corpus_df = corpus_df.loc[list(test_doc_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "faiss_path = \"faiss\"\n",
    "\n",
    "if not os.path.exists(faiss_path):\n",
    "    os.mkdir(faiss_path)\n",
    "\n",
    "silver_encoder_model = \"ipipan/silver-retriever-base-v1\"\n",
    "e5_large_encoder_model = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "\n",
    "def get_retriever(document_store: FAISSDocumentStore, model: str) -> DensePassageRetriever:\n",
    "    return DensePassageRetriever(\n",
    "        document_store=document_store,\n",
    "        query_embedding_model=model,\n",
    "        passage_embedding_model=model,\n",
    "        use_gpu=torch.cuda.is_available(),\n",
    "        embed_title=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_retriever(index_name: str, model: str, embedding_dim: int) -> DensePassageRetriever:\n",
    "    index_path = f\"{faiss_path}/{index_name}\"\n",
    "\n",
    "    try:\n",
    "        document_store = FAISSDocumentStore(\n",
    "            sql_url=f\"sqlite:///{index_path}_faiss_document_store.d\",\n",
    "            faiss_index_factory_str=\"Flat\",\n",
    "            return_embedding=True,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "\n",
    "        retriever = get_retriever(document_store, model)\n",
    "\n",
    "        passages_json = [\n",
    "            {\"content\": row[\"text\"], \"meta\": {\"fiqa_id\": idx}} \n",
    "            for idx, row in test_corpus_df.iterrows()\n",
    "        ]\n",
    "\n",
    "        document_store.write_documents(passages_json)\n",
    "        document_store.update_embeddings(retriever=retriever)\n",
    "        document_store.save(index_path)\n",
    "    except:\n",
    "        document_store = FAISSDocumentStore.load(index_path)\n",
    "\n",
    "        retriever = get_retriever(document_store, model)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\jupiterb\\agh-data-science\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Using a model of type 'xlm-roberta' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.\n",
      "Using a model of type 'xlm-roberta' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.\n"
     ]
    }
   ],
   "source": [
    "silver_retriever = prepare_retriever(\"ds_silver\", silver_encoder_model, 768)\n",
    "e5_large_retriever = prepare_retriever(\"ds_e5_large\", e5_large_encoder_model, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5-6 and 8\n",
    "Use the set of questions and the scorings defined in this corpus, to compute NDCG@5 for the dense retriever.\n",
    "\n",
    "Use a different dense encoder, e.g. E5 large or Polish Roberta Base and compute NDCG@5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_pipe = DocumentSearchPipeline(retriever=silver_retriever)\n",
    "e5_large_pipe = DocumentSearchPipeline(retriever=e5_large_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SearchEngine(ABC):\n",
    "    @abstractmethod\n",
    "    def get_top_searches(self, query: str, limit: int) -> pd.DataFrame:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRSearchEngine(SearchEngine):\n",
    "    def __init__(self, pipe: DocumentSearchPipeline) -> None:\n",
    "        self._pipe = pipe\n",
    "\n",
    "    def get_top_searches(self, query: str, limit: int) -> pd.DataFrame:\n",
    "        prediction = self._pipe.run(query=query, params={\"Retriever\": {\"top_k\": limit}})\n",
    "\n",
    "        ids = [doc.meta[\"fiqa_id\"] for doc in prediction[\"documents\"]]\n",
    "        docs = [doc.content for doc in prediction[\"documents\"]]\n",
    "\n",
    "        return pd.DataFrame({\"id\": ids, \"text\": docs}).set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_dpr_search_engine = DPRSearchEngine(silver_pipe)\n",
    "e5_large_dpr_search_engine = DPRSearchEngine(e5_large_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Jak zdeponować czek wystawiony na współpracownika w mojej firmie na moje konto firmowe?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65404</th>\n",
       "      <td>Po prostu poproś współpracownika o podpisanie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342212</th>\n",
       "      <td>Byłem właścicielem, a także najemcą. Mogłem wp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29372</th>\n",
       "      <td>„Powiedzmy, że jesteś mi winien 123,00 USD i c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213331</th>\n",
       "      <td>„Twój przyjaciel prawdopodobnie nie może wpłac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566392</th>\n",
       "      <td>Poproś o ponowne wystawienie czeku właściwemu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73427</th>\n",
       "      <td>Środki zarobione i wydane przed otwarciem dedy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64138</th>\n",
       "      <td>„Wypisałbym im czek lub wręczyłbym im gotówkę....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555486</th>\n",
       "      <td>„1.Dlaczego nie ma adnotacji „„Skarbu Stanów Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296769</th>\n",
       "      <td>Zwykle otrzymuję czek kasjerski na pokrycie ok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108739</th>\n",
       "      <td>Możesz zapłacić czekiem kasjerskim lub czekiem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "id                                                       \n",
       "65404   Po prostu poproś współpracownika o podpisanie ...\n",
       "342212  Byłem właścicielem, a także najemcą. Mogłem wp...\n",
       "29372   „Powiedzmy, że jesteś mi winien 123,00 USD i c...\n",
       "213331  „Twój przyjaciel prawdopodobnie nie może wpłac...\n",
       "566392  Poproś o ponowne wystawienie czeku właściwemu ...\n",
       "73427   Środki zarobione i wydane przed otwarciem dedy...\n",
       "64138   „Wypisałbym im czek lub wręczyłbym im gotówkę....\n",
       "555486  „1.Dlaczego nie ma adnotacji „„Skarbu Stanów Z...\n",
       "296769  Zwykle otrzymuję czek kasjerski na pokrycie ok...\n",
       "108739  Możesz zapłacić czekiem kasjerskim lub czekiem..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = queries_df.iloc[8][\"text\"]\n",
    "\n",
    "print(f\"Query: {_query}\")\n",
    "\n",
    "silver_dpr_search_engine.get_top_searches(_query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65404</th>\n",
       "      <td>Po prostu poproś współpracownika o podpisanie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525200</th>\n",
       "      <td>Nie zrobiłbym tego. Istnieje ryzyko, że Twój c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566392</th>\n",
       "      <td>Poproś o ponowne wystawienie czeku właściwemu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590102</th>\n",
       "      <td>Kiedy firma prosi mnie o wystawienie czeku na ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267362</th>\n",
       "      <td>Sprawdź oszustwo. Firmy, które mają nieodebran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29372</th>\n",
       "      <td>„Powiedzmy, że jesteś mi winien 123,00 USD i c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220691</th>\n",
       "      <td>W Wielkiej Brytanii oficjalną zasadą jest to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342212</th>\n",
       "      <td>Byłem właścicielem, a także najemcą. Mogłem wp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89326</th>\n",
       "      <td>Czeki są zwykle numerowane sekwencyjnie, aby z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388147</th>\n",
       "      <td>„Możesz spróbować napisać na odwrocie czeku, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "id                                                       \n",
       "65404   Po prostu poproś współpracownika o podpisanie ...\n",
       "525200  Nie zrobiłbym tego. Istnieje ryzyko, że Twój c...\n",
       "566392  Poproś o ponowne wystawienie czeku właściwemu ...\n",
       "590102  Kiedy firma prosi mnie o wystawienie czeku na ...\n",
       "267362  Sprawdź oszustwo. Firmy, które mają nieodebran...\n",
       "29372   „Powiedzmy, że jesteś mi winien 123,00 USD i c...\n",
       "220691  W Wielkiej Brytanii oficjalną zasadą jest to, ...\n",
       "342212  Byłem właścicielem, a także najemcą. Mogłem wp...\n",
       "89326   Czeki są zwykle numerowane sekwencyjnie, aby z...\n",
       "388147  „Możesz spróbować napisać na odwrocie czeku, w..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e5_large_dpr_search_engine.get_top_searches(_query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class NDCGBenchmark:\n",
    "    def __init__(\n",
    "        self, \n",
    "        queries: pd.DataFrame, \n",
    "        positive_qa: pd.DataFrame, \n",
    "    ) -> None:\n",
    "        self._queries = queries\n",
    "        self._scores_map = {}\n",
    "        \n",
    "        for _, row in positive_qa.iterrows():\n",
    "            query_id = row[\"query-id\"]\n",
    "            doc_id = row[\"corpus-id\"]\n",
    "            \n",
    "            if query_id not in self._scores_map:\n",
    "                self._scores_map[query_id] = dict()\n",
    "\n",
    "            self._scores_map[query_id][doc_id] = 1\n",
    "\n",
    "    def _eval_search_results(self, query_id: int, search_engine: SearchEngine, N: int) -> list[int]:\n",
    "        query = self._queries.loc[query_id][\"text\"]\n",
    "        results = search_engine.get_top_searches(query, N)\n",
    "        return [self._scores_map[query_id].get(corpus_id, 0) for corpus_id in results.index]\n",
    "    \n",
    "    def _eval_queries(self, search_engine: SearchEngine, N: int) -> np.ndarray:\n",
    "        num_queries = len(self._scores_map)\n",
    "        scores = np.empty((num_queries, N), dtype=int)\n",
    "\n",
    "        for i, query_id in tqdm(enumerate(self._scores_map), \"Eval queries\"):\n",
    "            scores[i] = self._eval_search_results(query_id, search_engine, N)\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def _target_scores(self, N: int) -> np.ndarray:\n",
    "        num_queries = len(self._scores_map)\n",
    "        scores = np.zeros((num_queries, N), dtype=int)\n",
    "\n",
    "        for i, targets in tqdm(enumerate(self._scores_map.values()), \"Eval targets\"):\n",
    "            num_targets = min(len(targets), N)\n",
    "            scores[i, :num_targets] = 1\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def mean_ndcg(self, search_engine: SearchEngine, N: int) -> float:\n",
    "        predictions = self._eval_queries(search_engine, N)\n",
    "        targets = self._target_scores(N)\n",
    "\n",
    "        dcg_weights = np.log2(np.arange(2, N + 2))\n",
    "        dcg_weights = np.resize(dcg_weights, predictions.shape)\n",
    "        dcg = np.sum(predictions / dcg_weights, axis=1)\n",
    "        idcg = np.sum(targets / dcg_weights, axis=1)\n",
    "        ndcg = dcg / idcg\n",
    "\n",
    "        return ndcg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_benchmark = NDCGBenchmark(queries_df, qa_test_df)\n",
    "\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval queries: 648it [00:39, 16.23it/s]\n",
      "Eval targets: 648it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for silver retriever base is: 0.43153572367707693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "silver_dpr_ndcg = ndcg_benchmark.mean_ndcg(silver_dpr_search_engine, N)\n",
    "\n",
    "print(f\"NDCG@{N} for silver retriever base is: {silver_dpr_ndcg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval queries: 648it [01:58,  5.45it/s]\n",
      "Eval targets: 648it [00:00, 648975.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for E5 large is: 0.3570620342046728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "e5_large_dpr_ndcg = ndcg_benchmark.mean_ndcg(e5_large_dpr_search_engine, N)\n",
    "\n",
    "print(f\"NDCG@{N} for E5 large is: {e5_large_dpr_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7\n",
    "Combine dense retrieval with classification model from lab 6 to implement a two-step retrieval. Compute NDCG@5 for this combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_query_and_doc(query: str, doc: str) -> str:\n",
    "    return f\"Pytanie: {query} Odpowiedź: {doc}\"\n",
    "\n",
    "\n",
    "class ClassifierSupportedSearchEngine(SearchEngine):\n",
    "    def __init__(\n",
    "        self, \n",
    "        search_engine: SearchEngine, \n",
    "        classifier: BertForSequenceClassification,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        num_candidates: int = 30,\n",
    "    ) -> None:\n",
    "        self._wrapped_engine = search_engine\n",
    "        self._classifier = classifier\n",
    "        self._tokenizer = tokenizer\n",
    "        self._num_candidates = num_candidates\n",
    "\n",
    "    def get_top_searches(self, query: str, limit: int) -> pd.DataFrame:\n",
    "        results = self._wrapped_engine.get_top_searches(\n",
    "            query, max(self._num_candidates, limit)\n",
    "        )\n",
    "        re_ranked_results = self._re_rank(query, results)\n",
    "        return re_ranked_results.head(limit)\n",
    "    \n",
    "    def _re_rank(self, query: str, results: pd.DataFrame) -> pd.DataFrame:\n",
    "        data = []\n",
    "        scores = {}\n",
    "        texts = []\n",
    "\n",
    "        for id, row in results.iterrows():\n",
    "            doc = row[\"text\"] \n",
    "            data.append({\"id\": id, \"text\": doc})\n",
    "            text = merge_query_and_doc(query, doc)\n",
    "            texts.append(text)\n",
    "            \n",
    "        tokens = self._tokenizer(\n",
    "            texts, \n",
    "            max_length=512, \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self._classifier(**tokens)\n",
    "\n",
    "        for id, score in zip(results.index, outputs.logits):\n",
    "            scores[id] = score[1].item()\n",
    "\n",
    "        data = sorted(data, key=lambda item: scores[item[\"id\"]], reverse=True)\n",
    "\n",
    "        return pd.DataFrame(data).set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "path_to_best = f\"models/qa_classifier/output/checkpoint-9000\"\n",
    "\n",
    "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    path_to_best, num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"allegro/herbert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_search_engine = ClassifierSupportedSearchEngine(\n",
    "    silver_dpr_search_engine, fine_tuned_model, tokenizer, num_candidates=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval queries: 648it [2:05:26, 11.61s/it]\n",
      "Eval targets: 648it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for silver retriever base re-ranked by sequence classifier is: 0.4545921502572553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_re_ranked_silver_dpr_ndcq = ndcg_benchmark.mean_ndcg(classifier_search_engine, N)\n",
    "\n",
    "print(f\"NDCG@{N} for silver retriever base re-ranked by sequence classifier is: {classifier_re_ranked_silver_dpr_ndcq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the methods: lexical match (e.g. ElasticSearch) or dense representation works better?\n",
    "\n",
    "I achieved the highest NDCG@5 for Silver Retriever Base (about 0.432) - it is quite better than NDCG@5 for ElasticSearch from lab6 (a.401). However, E5 large was achieved the lowest score - about 0.357 NDCG@5. It is interesting that re-ranking with classification model from lab6 gives better result with ElasticSearch than with Silver Retriever Base as a first step search (0.490 vs 0455 NDCG@5)\n",
    "\n",
    "## Which of the methods is faster?\n",
    "\n",
    "It of course depends on the model size, but DPR is slower method than ElasticSearch in most cases.\n",
    "\n",
    "## Try to determine the other pros and cons of using lexical search and dense document retrieval models.\n",
    "\n",
    "Lexical search offers poor handling of synonyms and variants - we need to specify them manually. DPR models are able to catch these cases, and in general are better in semantic understanding. On the other hand, these models cost more than simple lexical search - we need to fine tune them on proper data, while the increase in results could be low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
